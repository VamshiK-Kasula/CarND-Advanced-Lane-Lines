
# **Advanced Lane Finding Project** 

---

Main goal: Finding Lanes.

Challenges: 
* Images from the camera are distorted
* Images taken at bad lighting conditions
* Difficult to detect lanes because of the lane color

---
The above mentioned challenges make it harder to find the lanes from the camera images. Camera images are processed in the following manner to find the lanes from the raw camera images.

* **Camera calibration:** Computing the camera calibration matrix and distortion coefficients from the given set of chessboard images and applying the calculated distortion correction to the raw images.
* **Gradients and color transforms:** Using color transforms, gradients, etc., to create a thresholded binary image.
* **Perspective Transformation:** Applying a perspective transform to rectify perspective distortion on binary image ("birds-eye view").
* **Lane Detection:** Detecting lane pixels and applying a polynomial fit to find the lane boundary.
* **Estimating the curvature:** Determining the curvature of the lane and lane positions with respect to car.
* **Visualizing lanes and calculated curvature:** Warping the detected lane boundaries back onto the original image.


[//]: # (Image References)

[undist_checker_board]: ./output_images/undistorted/Checker_board.jpg "Undistorted Checker Board"
[undist_test_img]: ./output_images/undistorted/test_img.jpg "Undistorted Test image"
[c_spaces_img1]: ./output_images/color_spaces/img1.jpg "Test image"
[c_spaces_img1_sobel]: ./output_images/color_spaces/img1_sobel.jpg "Sobel-x"
[c_spaces_img1_s_space]: ./output_images/color_spaces/img1_s_space.jpg "S space from HLS"
[c_spaces_img1_combined]: ./output_images/color_spaces/img1_combined.jpg "Sobel and S combined"
[c_spaces_img2]: ./output_images/color_spaces/img2.jpg "Test image"
[c_spaces_img2_sobel]: ./output_images/color_spaces/img2_sobel.jpg "Sobel-x"
[c_spaces_img2_s_space]: ./output_images/color_spaces/img2_s_space.jpg "S space from HLS"
[c_spaces_img2_combined]: ./output_images/color_spaces/img2_combined.jpg "Sobel and S combined"
[perspective_transform]: ./output_images/perspective_transform/perspective_transform.png "Perspective Transformation"
[ROC]: ./output_images/test3.jpg "dDetected Lanes and ROC"
[pipeline_image]: ./output_images/fig_test4.jpg "Pipeline image"

[lanes]: ./output_images/lane_detection/lane_detection.jpg "Lane Detection"
[video1]: ./project_video.mp4 "Video"

---
## Pipeline

#### **1. Camera calibration:**
The images from the cameras are generally distorted either because of the lens defects or the improper alignment of the camera.

In this step, distortion coefficients of the camera are calculated. This is estimated by taking multiple images of a known checker board pattern aligned in various positions.

Corners in these images are obtained from the OpenCv function 'cv2.findChessboardCorners'. The actual corners of the chessboard grid (called as `objpoints`) are associated to the corners in the images from the camera (called as `imgpoints`). The distortion coefficients are calculated by feeding the arrays  `objpoints` and `imgpoints` to the function `cv2.calibrateCamera()`. The correction is applied to the images using the `cv2.undistort()` function.


![alt text][undist_checker_board]

![alt text][undist_test_img]


#### **2. Gradients and color transforms:** 


#### (i) Sobel Operator
* A gradiant image is generated by applying sobel operator in x and y direction on a grascale image and finding the manitude of the sobel.
* This is preferred to canny edge detection produces a lot of insignificant edges are found sing canny edge detection.

#### (ii) Color spaces
* As discussed in the course, choosing a right color space can be useful in extracting desired information from a particular image. For instance, yellow colored lane markings are not so strongly identified in grayscale from RGB image. But yello color can be identified easily in the S-Channel of HLS color space.

Sobel operator is effective in detecting edges in the darker regions of the image. Whereas, s-channel in HLS is preferred for yellow colored lane lines.

Hence a combination of color and gradient thresholded images are used to generate a binary image.

| Image| Sobel Transform| S space| Combined|
|:-:|:-:|:-:|:-:|
|![alt text][c_spaces_img1]|![alt text][c_spaces_img1_sobel]|![alt text][c_spaces_img1_s_space]|![alt text][c_spaces_img1_combined]|
|![alt text][c_spaces_img2]|![alt text][c_spaces_img2_sobel]|![alt text][c_spaces_img2_s_space]|![alt text][c_spaces_img2_combined]|

#### **3. Perspective Transformation:**

In this step, the generated binary image is transformed to a bird's eye view perspective.

Manually source points (Region of Interest) and destination points are chosen. This makes the lanes to be parallel in the bird's eye view.



following hardcoded source and destination points:

| Source        | Destination   | 
|:-------------:|:-------------:| 
| [260, 655 ]   | [280, 655 ]   | 
| [560, 470 ]   | [280, 0   ]   |
| [730, 470 ]   | [1020, 0  ]   |
| [1040, 655]   | [1020, 655]   |



![alt text][perspective_transform]

#### **4. Lane Detection:**

The lanes are now identified from the thresholded warped image by applying sliding window search in x-direction as described in the lecture. By applying the sliding window, peaks in histogram are identified and these are marked as left and right lane pixels.

A second degree polynomial is estimated for the left and right pixel groups using the `numpy.polyfit()` function.

![alt text][lanes]

#### **5. Estimating the curvature:**

Following conversions in x and y from image space to realworld space are used
* `ym_p = 30/720` # meters per pixel in y dimension
* `xm_p = 3.7/700` # meters per pixel in x dimension

with
* `y_eval = np.max(ploty) * ym_p`  # `np.max(ploty)` is simply the maximum height of the image
* `camera_pos = (img_len_x/2) * xm_p` # mounted camera position in real world coordinates
* `A = fitx[0]*xm_p/(ym_p**2)` # fitx is the calculated polynomial fit of the lane
* `B = fitx[1]*(xm_p/ym_p)`
* `C = fitx[2]*xm_p`

Radius of curvature is estimated as follows

    radius_of_curvature = ((1 + (2*A*y_eval +B)**2)**1.5) / np.absolute(2*A)

and, the distance of the lane from the center/car is calculated as

    dist_center = A*y_eval**2+B*y_eval+C-camera_pos

![alt text][ROC]

#### **6. Visualizing lanes and calculated curvature:**

THe said steps are implemented as a pipeline and he desired results are achieved. Pipeline also works exceptionally well on the video stream.

![alt text][pipeline_image]

### Pipeline (video)



Here's a [link to my video result](https://www.youtube.com/watch?v=cU1ItS24-_c&feature=youtu.be)

---

### Discussion

Although the pipeline works pretty well on the provided images and the `project_video.mp4`, there is a scope for improvement in the following areas.

* Region of interest for birds eye view (perspective transform) is selected manually. May be a simple lane line algorithm designed in te earlier project can be used to determine the region where the lanes are present in the image.
* In the video stream, if there is a sudden change of lighting conditions or road color, the pipeline is not accurate through this region.
* In the other challenge videos, the road has patches due to the construction/repairs. As the patches have different color compared to the road, the borders of the patches are intern perceived as the lanes by the pipeline.
  * more intelligence is to be provided to the pipeline .
  * a simple solution can be simply selecting the detected lanes in the region of interest by considering the minimum distance between the lanes.